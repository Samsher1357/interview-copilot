# AI Interview Copilot

A professional-grade, **bidirectional** AI Interview Copilot that provides real-time assistance for **both interviewers AND applicants**. Listen to live conversations, get instant AI-powered insights, and conduct better interviews with role-specific guidance.

## ğŸ†• What's New in v2.0

- **ğŸ¯ Complete Conversation Analysis**: Analyzes BOTH interviewer and applicant speech
- **ğŸ’¡ Dual Assistance**: 
  - When interviewer speaks â†’ Get answer suggestions
  - When you speak â†’ Get feedback and improvements
- **ğŸ—ï¸ Improved Architecture**: Clean service layer with better scalability
- **ğŸ“¡ Real-time Events**: WebSocket-based event system for instant updates
- **âš™ï¸ Configuration System**: Centralized settings management

**[See IMPROVEMENTS.md for full details â†’](IMPROVEMENTS.md)**

## Features

### Core Capabilities
- ğŸ¤ **Real-time Audio Transcription**: Continuously transcribes spoken dialogue from both parties using Deepgram (professional-grade speech recognition)
- ğŸ¤– **AI-Powered Analysis**: Detects interviewer intent (evaluation, follow-up questions, role-specific topics) using LangChain.js
- âš¡ **Real-time Streaming**: AI responses stream in real-time as they're generated for instant feedback
- ğŸŒ **Multi-language Support**: Supports transcription and response generation in multiple languages
- ğŸ¨ **Modern UI**: Clean, responsive interface with dark mode support
- ğŸ”’ **Secure API Keys**: Server-side API key handling for improved security

### ğŸ†• NEW: Complete Conversation Analysis
- ğŸ¤ **Listen to Everyone**: Analyzes both interviewer and applicant speech in real-time
- ğŸ’¡ **Answer Suggestions**: When interviewer asks questions, get complete ready-to-use answers
- ğŸ”„ **Live Feedback**: When you answer, get instant feedback and improvement suggestions
- â“ **Question Help**: When you ask questions, get suggestions to make them better
- ğŸ¤” **Clarification Assistance**: When you need clarification, get help phrasing it professionally
- ğŸ“Š **What to Add Next**: Real-time hints on what points to mention or elaborate on
- ğŸ¯ **Context-Aware**: Uses conversation context to provide relevant assistance

### ğŸ†• NEW: Advanced Architecture
- ğŸ—ï¸ **Service Layer**: Clean separation between UI, business logic, and services
- ğŸ“¡ **Real-time Events**: WebSocket-based event system for instant updates
- âš™ï¸ **Configuration Management**: Centralized settings with persistence
- ğŸ”§ **Testable Code**: Independent service layer for better maintainability
- ğŸš€ **Multi-user Ready**: Event-driven architecture supports future multi-user scenarios

### Additional Features
- ğŸ’¡ **Intelligent Responses**: Generates contextual and professional suggestions, hints, and talking points
- ğŸ”Š **Auto-Speak**: Optional text-to-speech for AI responses
- ğŸ“Š **Context-Aware**: Uses interview context (job role, skills, experience) for personalized responses
- ğŸ“ **Multiple Use Cases**: Job interviews, practice sessions, training, evaluation assistance

## Prerequisites

- Node.js 18+ and npm/yarn
- OpenAI API key (for AI analysis and response generation)
- Deepgram API key (for speech recognition) - [Get free API key](https://console.deepgram.com)
- Modern browser with microphone access (Chrome, Edge, Firefox, or Safari)

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd interview-copilot
```

2. Install dependencies:
```bash
npm install
```

3. Create a `.env.local` file in the root directory:

**Option A: Using OpenAI (Default)**
```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
AI_PROVIDER=openai

# OpenAI Model Selection (optional, default: gpt-4o-mini)
# Options: gpt-3.5-turbo (cheapest, higher limits), gpt-4o-mini (default), gpt-4, gpt-4o
OPENAI_MODEL=gpt-4o-mini

# Maximum tokens for AI responses (optional, default: 1200)
AI_MAX_TOKENS=1200

# Deepgram API Key (for speech recognition)
DEEPGRAM_API_KEY=your_deepgram_api_key_here

NEXT_PUBLIC_AUTO_SPEAK=false
```

**Option B: Using Google Gemini (Free, Fast!)**
```env
# Google Gemini Configuration
GOOGLE_API_KEY=your_google_api_key_here
AI_PROVIDER=gemini

# Gemini Model Selection (optional, default: gemini-1.5-flash)
# Options: gemini-1.5-flash (free, fastest), gemini-1.5-pro (more powerful, free tier)
GEMINI_MODEL=gemini-1.5-flash

# Maximum tokens for AI responses (optional, default: 1200)
AI_MAX_TOKENS=1200

# Deepgram API Key (for speech recognition)
DEEPGRAM_API_KEY=your_deepgram_api_key_here

NEXT_PUBLIC_AUTO_SPEAK=false
```

**Getting API Keys:**
- **OpenAI:** https://platform.openai.com/api-keys
- **Google Gemini (FREE):** https://makersuite.google.com/app/apikey
- **Deepgram (FREE):** https://console.deepgram.com

**Troubleshooting Rate Limits:**
If you see "Rate limit reached" errors:
- **Option 1:** Switch to Gemini: `AI_PROVIDER=gemini` (FREE!)
- **Option 2:** Use cheaper OpenAI model: `OPENAI_MODEL=gpt-3.5-turbo`
- **Option 3:** Reduce tokens: `AI_MAX_TOKENS=800`
- **Option 4:** Add payment method at [OpenAI Billing](https://platform.openai.com/account/billing)
- **Option 5:** Wait for quota to reset (shown in error message)

**Note**: 
- For better security, use `OPENAI_API_KEY` (server-side only) instead of `NEXT_PUBLIC_OPENAI_API_KEY`
- Deepgram API key is required for speech recognition. Get a free API key at [Deepgram Console](https://console.deepgram.com)
- The app uses Deepgram for professional-grade speech recognition with better accuracy than browser Web Speech API

4. Run the development server:
```bash
npm run dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser

## Usage

### Quick Start

1. **Configure Context** (Optional): Add your job role, skills, and relevant information
2. **Start Listening**: Click the "Start Listening" button to begin transcription
3. **Get Real-time Assistance**: AI analyzes everyone's speech and provides help
   - Interviewer speaks â†’ Get answer suggestions
   - You speak â†’ Get feedback and improvements

### How It Works

#### When Interviewer Speaks
The AI provides **answer suggestions**:
- âœ… Complete, ready-to-use answers
- ğŸ’¡ Key talking points to mention
- ğŸ“ Structured responses (STAR method)
- ğŸ¯ Examples and elaborations

#### When You (Applicant) Speak

The AI detects what you're doing and provides **appropriate help**:

**If you're ANSWERING:**
- ğŸ”„ What you did well
- ğŸ’¡ What to add or elaborate on
- ğŸ“Š Points you missed
- ğŸ¯ How to improve your answer

**If you're ASKING A QUESTION:**
- âœ… Quality of your question
- ğŸ’¡ How to make it better/more specific
- â“ Follow-up questions to consider
- â° Timing suggestions

**If you're ASKING FOR CLARIFICATION:**
- âœ… Validates it's good to clarify
- ğŸ’¬ Better ways to phrase it
- ğŸ¯ What specifically to clarify
- ğŸ”„ What to say after getting clarification

**Example Flow:**
```
1. Interviewer: "Tell me about a challenging project."
   â†’ AI shows: Complete STAR answer, talking points

2. You start answering...
   â†’ AI shows: "Good start! Also mention: team size, specific metrics"

3. You: "Could you clarify what you mean by challenging?"
   â†’ AI shows: "Good question! You could also ask about: technical vs organizational challenges"

4. You: "What's the team structure for this role?"
   â†’ AI shows: "Excellent question! Follow up with: reporting structure, collaboration style"
```

### Additional Features

1. **Select Language**: Choose your preferred language from the dropdown
2. **View Transcripts**: See real-time transcriptions with speaker identification
3. **Auto-Speak**: Enable auto-speak to have AI suggestions read aloud automatically
4. **Manual Speak**: Click the speaker icon on any response to hear it spoken
5. **Export Session**: Export transcripts and AI responses for later review

## Supported Languages

- English (US/UK)
- Spanish
- French
- German
- Italian
- Portuguese
- Japanese
- Chinese (Simplified)
- Korean

## Browser Compatibility

- âœ… Chrome/Edge (recommended)
- âœ… Safari
- âœ… Firefox
- âœ… All modern browsers with microphone access support

**Note**: The app uses Deepgram for speech recognition, which works in all modern browsers (unlike Web Speech API which has limited browser support).

## Project Structure

```
interview-copilot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx                   # Root layout
â”‚   â”œâ”€â”€ page.tsx                     # Main page
â”‚   â”œâ”€â”€ globals.css                  # Global styles
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ analyze-stream/          # Streaming analysis endpoint (role-aware)
â”‚       â”œâ”€â”€ deepgram/                # Deepgram WebSocket endpoint
â”‚       â”œâ”€â”€ parse-pdf/               # PDF parsing
â”‚       â”œâ”€â”€ parse-resume/            # Resume parsing
â”‚       â””â”€â”€ socket/                  # Socket.IO endpoint
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ InterviewCopilot.tsx         # Main component
â”‚   â”œâ”€â”€ RoleSelector.tsx             # ğŸ†• Role selection UI
â”‚   â”œâ”€â”€ DeepgramTranscriber.tsx     # Speech recognition (role-aware)
â”‚   â”œâ”€â”€ ControlPanel.tsx            # Control buttons
â”‚   â”œâ”€â”€ TranscriptPanel.tsx         # Transcript display
â”‚   â”œâ”€â”€ ResponsePanel.tsx           # AI responses display
â”‚   â””â”€â”€ ContextModal.tsx            # Context settings
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ store.ts                     # Zustand state management (with role)
â”‚   â”œâ”€â”€ langchainService.ts         # LangChain.js with role support
â”‚   â”œâ”€â”€ deepgramService.ts          # Deepgram client service
â”‚   â”œâ”€â”€ socket-server.ts            # Socket.IO server with events
â”‚   â”œâ”€â”€ services/                    # ğŸ†• Service Layer
â”‚   â”‚   â”œâ”€â”€ TranscriptionService.ts  # Deepgram management
â”‚   â”‚   â”œâ”€â”€ AIAnalysisService.ts     # Role-aware AI analysis
â”‚   â”‚   â”œâ”€â”€ RolePromptStrategy.ts    # Role-specific prompts
â”‚   â”‚   â”œâ”€â”€ ConfigurationService.ts  # Settings management
â”‚   â”‚   â””â”€â”€ RealtimeEventService.ts  # WebSocket events
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ useDeepgram.ts           # Deepgram client hook
â”‚       â”œâ”€â”€ useSocket.ts             # Socket.IO client hook
â”‚       â””â”€â”€ useStreamingAnalysis.ts  # Streaming analysis (role-aware)
â”œâ”€â”€ ARCHITECTURE.md                   # ğŸ†• Architecture documentation
â”œâ”€â”€ IMPROVEMENTS.md                   # ğŸ†• Improvements summary
â””â”€â”€ package.json
```

## Technology Stack

### Core Technologies
- **Next.js 14**: React framework with App Router
- **TypeScript**: Type-safe development
- **Tailwind CSS**: Utility-first styling

### AI & Speech
- **LangChain.js**: AI orchestration with streaming support
- **OpenAI / Gemini**: Flexible AI provider support
- **Deepgram**: Professional-grade real-time speech recognition with speaker diarization

### Real-time & State
- **Socket.IO**: Real-time bidirectional communication
- **Zustand**: Lightweight state management with persistence
- **WebSocket**: Low-latency event streaming

### Architecture
- **Service Layer Pattern**: Clean separation of concerns
- **Event-Driven Architecture**: Pub/sub pattern for real-time updates
- **Role-Based Strategies**: Adaptive AI prompts based on user role

## Configuration

### Environment Variables

#### AI Provider Selection
- `AI_PROVIDER`: Choose AI provider (optional, default: `openai`)
  - `openai`: Use OpenAI models (GPT)
  - `gemini`: Use Google Gemini models (FREE!)

#### OpenAI Configuration (if using OpenAI)
- `OPENAI_API_KEY`: Your OpenAI API key (required for OpenAI)
- `OPENAI_MODEL`: AI model to use (optional, default: `gpt-4o-mini`)
  - `gpt-3.5-turbo`: Cheapest, highest rate limits, good quality
  - `gpt-4o-mini`: Balanced (default)
  - `gpt-4`: Most powerful, expensive
  - `gpt-4o`: Latest, powerful

#### Google Gemini Configuration (if using Gemini)
- `GOOGLE_API_KEY`: Your Google API key (required for Gemini) - **FREE!**
  - Get at: https://makersuite.google.com/app/apikey
- `GEMINI_MODEL`: Gemini model to use (optional, default: `gemini-1.5-flash`)
  - `gemini-1.5-flash`: Free, fastest, great quality (recommended)
  - `gemini-1.5-pro`: More powerful, free tier available
  - `gemini-2.0-flash-exp`: Experimental, latest features

#### Common Configuration
- `AI_MAX_TOKENS`: Max response length (optional, default: `1200`)
- `DEEPGRAM_API_KEY`: Your Deepgram API key (required for speech recognition)
- `NEXT_PUBLIC_AUTO_SPEAK`: Enable/disable auto-speak by default (optional, default: false)

**Security Note**: 
- Use `OPENAI_API_KEY` for production. It's server-side only and more secure than `NEXT_PUBLIC_OPENAI_API_KEY`
- Deepgram API key is handled server-side for security

### Customization

You can customize the AI behavior by modifying the system prompt in `lib/langchainService.ts`. The prompt controls how the AI analyzes conversations and generates responses.

## Security Notes

- Never commit your `.env.local` file or API keys to version control
- **Use `OPENAI_API_KEY` instead of `NEXT_PUBLIC_OPENAI_API_KEY`** for better security (server-side only)
- All API calls are now made server-side through Next.js API routes
- For production, implement rate limiting and API key protection
- Socket.IO connections are configured with CORS protection

## Troubleshooting

### Speech Recognition Not Working

- Ensure you're using a supported browser (Chrome/Edge/Safari)
- Check microphone permissions in your browser settings
- Make sure you're using HTTPS (required for microphone access in most browsers)
- Try refreshing the page and allowing microphone access when prompted

### AI Responses Not Appearing

- **Check API Key**: Verify your OpenAI API key is correctly set in `.env.local`
  - The key should start with `sk-`
  - Make sure there are no extra spaces or quotes
  - Restart the dev server after changing `.env.local`
  
- **Check Browser Console**: Open Developer Tools (F12) and check for errors
  - Look for API errors or network failures
  - Check if the `/api/analyze` endpoint is being called
  
- **Verify API Credits**: Ensure you have sufficient OpenAI API credits
  - Check your OpenAI dashboard at https://platform.openai.com
  
- **Check Network**: Ensure you have internet connection
  - The app needs to connect to OpenAI's API

### Common Issues

1. **"OpenAI API key not configured" error**
   - Make sure `.env.local` exists in the root directory
   - Verify the variable name is exactly `NEXT_PUBLIC_OPENAI_API_KEY`
   - Restart the dev server: `npm run dev`

2. **No answers appearing when interviewer speaks**
   - Check if transcripts are appearing (if not, microphone issue)
   - Look for error messages in the error display (top-right)
   - Check browser console for detailed errors

3. **API errors (401, 403, 429)**
   - 401: Invalid API key - check your key
   - 403: API key doesn't have access - check your OpenAI account
   - 429: Rate limit exceeded - wait a moment and try again

4. **"Analyzing..." but no results**
   - Check browser console for errors
   - Verify API key is valid
   - Check network tab to see if API calls are failing

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

